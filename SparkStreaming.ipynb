{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install gcsfs -q"
      ],
      "metadata": {
        "id": "OjiFf_22rObq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiiR_bv7rlkm",
        "outputId": "e9ef29e0-1f6e-4f71-c76b-11c75cb7bb78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-24 16:14:55--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar.5’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  32.26M  14.0MB/s    in 2.3s    \n",
            "\n",
            "2025-01-24 16:14:58 (14.0 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar.5’ saved [33831577/33831577]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output/*"
      ],
      "metadata": {
        "id": "nSX9g3fQ-stw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZa7qEBKh3lM",
        "outputId": "e5fb5248-2e6e-4486-cbbc-52fd92583616"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The environment variable [GOOGLE_APPLICATION_CREDENTIALS] is set to:\n",
            "  [/content/.config/application_default_credentials.json]\n",
            "Credentials will still be generated to the default location:\n",
            "  [/content/.config/application_default_credentials.json]\n",
            "To use these credentials, unset this environment variable before\n",
            "running your application.\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=ruV1R4kNl9heplGz8c8iulxc8RC43s&prompt=consent&token_usage=remote&access_type=offline&code_challenge=w42DSLjhmhvlanV1yyFC2gOsM_DQK95UYB6DQVCcp6k&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0ASVgi3KI-2WXej_-HHxsrDYUe-w-RpwpiGuVXJFsnQLrjuEeHmNAl4gfz-eXPg6qMB51Tw\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot add the project \"data-eng-dev-437916\" to ADC as the quota project because the account in ADC does not have the \"serviceusage.services.use\" permission on this project. You might receive a \"quota_exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
        "from pyspark.sql.functions import col, count, sum, lit, window, first, sqrt, pow, last, when\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "\n",
        "#Create the Spark session with the GCS connector\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCSReadVehicles\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path to the credentials file generated by gcloud\n",
        "credential_path = \"/content/.config/application_default_credentials.json\"\n",
        "\n",
        "# Set up the environment for credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_path\n",
        "\n",
        "# Configure PySpark to access GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", credential_path)\n",
        "\n",
        "# GCS bucket path\n",
        "bucket_path = \"gs://edit-de-project-streaming-data/carris-vehicles/\"\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", LongType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", DoubleType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True),\n",
        "])\n",
        "\n",
        "# 1st Query: Read data from GCS and write to the output/bronze directory\n",
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 5) \\\n",
        "    .load(bucket_path)\n",
        "\n",
        "# Path to the folder where data will be written\n",
        "bronze_output_path = \"output/bronze\"\n",
        "#To save data in bucket\n",
        "#bronze_output_path = \"gs://edit-data-eng-project-group2/vehicles_data/\"\n",
        "\n",
        "# Ensure the timestamp column is in the correct format\n",
        "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "\n",
        "# Define a 2-minute window for aggregations\n",
        "agg_distance = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"lat\").alias(\"start_lat\"),\n",
        "        first(\"lon\").alias(\"start_lon\"),\n",
        "        last(\"lat\").alias(\"end_lat\"),\n",
        "        last(\"lon\").alias(\"end_lon\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "# Calculate distance using the Euclidean distance formula between the first and last point in the window\n",
        "agg_distance = agg_distance.withColumn(\n",
        "    \"distance\",\n",
        "    sqrt(\n",
        "        pow(col(\"end_lat\") - col(\"start_lat\"), 2) + pow(col(\"end_lon\") - col(\"start_lon\"), 2)\n",
        "    ) * lit(111)  # Simplified approximation (in km)\n",
        ")\n",
        "\n",
        "# Calculate the average speed (km/h) based on distance and time\n",
        "agg_speed = agg_distance.withColumn(\n",
        "    \"average_speed_kmh\",\n",
        "    (col(\"distance\") / lit(2 / 60))  # 2-minute time converted to hours\n",
        ")\n",
        "\n",
        "# Add logic to define current_stop or next_stop based on current_status\n",
        "df = df.withColumn(\n",
        "    \"stop\",\n",
        "    when(col(\"current_status\") == \"stopped_at\", col(\"stop_id\"))\n",
        "    .otherwise(lit(\"-1\"))\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"next_stop\",\n",
        "    when(col(\"current_status\") != \"stopped_at\", col(\"stop_id\"))\n",
        "    .otherwise(lit(\"-1\"))\n",
        ")\n",
        "\n",
        "# Aggregate vehicle attributes in the 2-minute window\n",
        "windowed_df = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"line_id\").alias(\"line\"),\n",
        "        first(\"route_id\").alias(\"route\"),\n",
        "        first(\"bearing\").alias(\"direction\"),\n",
        "        first(\"stop\").alias(\"current_stop\"),\n",
        "        first(\"next_stop\").alias(\"next_stop\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "windowed_df = windowed_df.drop(\"data_points\")\n",
        "\n",
        "# Join speed and distance data with vehicle attributes\n",
        "final_result = agg_speed.join(windowed_df, [\"id\", \"window\"], \"inner\")\n",
        "\n",
        "final_result = final_result.drop(\"data_points\")\n",
        "final_result = final_result.drop(\"start_lat\")\n",
        "final_result = final_result.drop(\"start_lon\")\n",
        "final_result = final_result.drop(\"end_lat\")\n",
        "final_result = final_result.drop(\"end_lon\")\n",
        "\n",
        "# Write to the output/bronze directory\n",
        "query_bronze = final_result.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", bronze_output_path) \\\n",
        "    .option(\"checkpointLocation\", f\"{bronze_output_path}/_checkpoint\") \\\n",
        "    .start()\n",
        "\n",
        "#To save data in bucket\n",
        "#def write_to_gcs(batch_df, batch_id):\n",
        "#    batch_df.write \\\n",
        "#        .mode(\"append\") \\\n",
        "#        .format(\"parquet\") \\\n",
        "#        .option(\"path\", \"gs://edit-data-eng-project-group2/vehicles_data/\") \\\n",
        "#        .save()\n",
        "\n",
        "#query_bronze = final_result.writeStream \\\n",
        "#    .outputMode(\"append\") \\\n",
        "#    .foreachBatch(write_to_gcs) \\\n",
        "#    .option(\"checkpointLocation\", \"gs://edit-data-eng-project-group2/vehicles_data/_checkpoint\") \\\n",
        "#    .trigger(processingTime=\"30 seconds\") \\\n",
        "#    .start()\n",
        "\n",
        "\n",
        "time.sleep(480)\n",
        "query_bronze.stop()\n",
        "# Wait until the streaming ends\n",
        "#query_bronze.awaitTermination()\n"
      ],
      "metadata": {
        "id": "eaz-Ephk_5fk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = spark.read.format(\"parquet\").load(\"/content/output/bronze\")\n",
        "output_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K0coKZbZ0sd",
        "outputId": "53d052a0-4270-4b5b-9a3a-d2c29a852e75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------+-----------------+----+-----+---------+---------+\n",
            "| id|window|distance|average_speed_kmh|line|route|direction|next_stop|\n",
            "+---+------+--------+-----------------+----+-----+---------+---------+\n",
            "+---+------+--------+-----------------+----+-----+---------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}