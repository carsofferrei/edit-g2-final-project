{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install gcsfs -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjiFf_22rObq",
        "outputId": "96677f1a-feeb-4dd7-d2c6-3aa050f4ea6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiiR_bv7rlkm",
        "outputId": "688b1656-0dd1-4c8a-9797-11d20ff5e450"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 19:56:06--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar.1’\n",
            "\n",
            "\r          gcs-conne   0%[                    ]       0  --.-KB/s               \rgcs-connector-hadoo 100%[===================>]  32.26M   197MB/s    in 0.2s    \n",
            "\n",
            "2025-01-23 19:56:06 (197 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar.1’ saved [33831577/33831577]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://edit-de-project-streaming-data/carris-vehicles/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jt7pE0g8Kn0",
        "outputId": "1ff3797e-755e-4dfc-db5c-e31d76d2d9e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ServiceException: 401 Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cat gs://edit-de-project-streaming-data/carris-vehicles/vehicles_1735510622.json"
      ],
      "metadata": {
        "id": "w5ex_27o52m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output/*"
      ],
      "metadata": {
        "id": "nSX9g3fQ-stw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZa7qEBKh3lM",
        "outputId": "8ae3c9b4-52d8-4784-b70a-9eeba2d2bc1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=paepnIJuJ2Kk34XtHHmL7NnAOC8mw7&prompt=consent&token_usage=remote&access_type=offline&code_challenge=5x-rrYdXQ052gceM2zuU4_Omris05UZYePemw0Fn3CY&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AanRRrttfCgZd6-kcAAChB1xhAPxQPfyOKBmL9N_zORog0Ehdb1mfSons2EbLR612jJDJA\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
        "from pyspark.sql.functions import col, avg, count, sum, lit\n",
        "import time\n",
        "\n",
        "# Create session Spark with conector GCS\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCSReadVehicles\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Default location of file generated by gcloud\n",
        "credential_path = \"/content/.config/application_default_credentials.json\"\n",
        "\n",
        "# Ambient variable configuration of credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_path\n",
        "\n",
        "# Config PySpark to access the GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", credential_path)\n",
        "\n",
        "\n",
        "# Bucket Path\n",
        "bucket_path = \"gs://edit-de-project-streaming-data/carris-vehicles/\"\n",
        "\n",
        "\n",
        "# Define schema (Used printSchema())\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", LongType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", DoubleType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Reading Json data from GCS\n",
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 5) \\\n",
        "    .load(\"gs://edit-de-project-streaming-data/carris-vehicles/\")\n",
        "\n",
        "# Path of folder to keep output from bucket\n",
        "output_path = \"/content/output\"\n",
        "#output_path = \"gs://edit-data-eng-project-group2/vehicles_data/\"\n",
        "\n",
        "# Writing in query and saving in output path\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", output_path) \\\n",
        "    .option(\"checkpointLocation\", \"/content/output/checkpoints/\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "#.option(\"checkpointLocation\", \"gs://edit-data-eng-project-group2/checkpoints/\") \\\n",
        "# 30 seconds of execution just for testing\n",
        "#time.sleep(30)\n",
        "#query.stop()\n"
      ],
      "metadata": {
        "id": "Lmzqp2-3rtg9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
        "from pyspark.sql.functions import col, count, sum, lit, window, first, sqrt, pow, last\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "\n",
        "# Cria a sessão Spark com o conector GCS\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCSReadVehicles\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho do arquivo de credenciais gerado pelo gcloud\n",
        "credential_path = \"/content/.config/application_default_credentials.json\"\n",
        "\n",
        "# Configuração do ambiente para as credenciais\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_path\n",
        "\n",
        "# Configura o PySpark para acessar o GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", credential_path)\n",
        "\n",
        "# Caminho do bucket no GCS\n",
        "bucket_path = \"gs://edit-de-project-streaming-data/carris-vehicles/\"\n",
        "\n",
        "# Define o esquema (usado para printSchema())\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", LongType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", DoubleType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True),\n",
        "])\n",
        "\n",
        "# 1ª Query: Lê os dados do GCS e escreve no diretório output/bronze\n",
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 5) \\\n",
        "    .load(bucket_path)\n",
        "\n",
        "# Caminho da pasta onde os dados brutos serão gravados\n",
        "bronze_output_path = \"output/bronze\"\n",
        "\n",
        "# Garantir que a coluna timestamp esteja no formato correto\n",
        "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "\n",
        "# Definir a janela de 2 minutos para agregações\n",
        "agg_distance = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"lat\").alias(\"start_lat\"),\n",
        "        first(\"lon\").alias(\"start_lon\"),\n",
        "        last(\"lat\").alias(\"end_lat\"),\n",
        "        last(\"lon\").alias(\"end_lon\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "# Calcular a distância utilizando a fórmula de distância euclidiana entre o primeiro e o último ponto na janela\n",
        "agg_distance = agg_distance.withColumn(\n",
        "    \"distance\",\n",
        "    sqrt(\n",
        "        pow(col(\"end_lat\") - col(\"start_lat\"), 2) + pow(col(\"end_lon\") - col(\"start_lon\"), 2)\n",
        "    ) * lit(111)  # Aproximação simplificada (em km)\n",
        ")\n",
        "\n",
        "# Calcular a velocidade média (km/h) com base na distância e tempo\n",
        "agg_speed = agg_distance.withColumn(\n",
        "    \"average_speed_kmh\",\n",
        "    (col(\"distance\") / lit(2 / 60))  # Tempo de 2 minutos convertido para horas\n",
        ")\n",
        "\n",
        "# Agregar atributos do veículo na janela de 2 minutos\n",
        "windowed_df = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"line_id\").alias(\"line\"),\n",
        "        first(\"route_id\").alias(\"route\"),\n",
        "        first(\"bearing\").alias(\"direction\"),\n",
        "        first(\"stop_id\").alias(\"next_stop\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "windowed_df = windowed_df.drop(\"data_points\")\n",
        "\n",
        "# Juntar os dados de velocidade e distância com os atributos do veículo\n",
        "final_result = agg_speed.join(windowed_df, [\"id\", \"window\"], \"inner\")\n",
        "\n",
        "final_result = final_result.drop(\"data_points\")\n",
        "final_result = final_result.drop(\"start_lat\")\n",
        "final_result = final_result.drop(\"start_lon\")\n",
        "final_result = final_result.drop(\"end_lat\")\n",
        "final_result = final_result.drop(\"end_lon\")\n",
        "\n",
        "# Grava no diretório output/bronze\n",
        "query_bronze = final_result.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", bronze_output_path) \\\n",
        "    .option(\"checkpointLocation\", f\"{bronze_output_path}/_checkpoint\") \\\n",
        "    .start()\n",
        "\n",
        "# Defina o esquema para os dados Parquet\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"start_lat\", DoubleType(), True),\n",
        "    StructField(\"start_lon\", DoubleType(), True),\n",
        "    StructField(\"end_lat\", DoubleType(), True),\n",
        "    StructField(\"end_lon\", DoubleType(), True),\n",
        "    StructField(\"data_points\", LongType(), True),\n",
        "    StructField(\"distance\", DoubleType(), True),\n",
        "    StructField(\"average_speed_kmh\", DoubleType(), True),\n",
        "    StructField(\"line\", StringType(), True),\n",
        "    StructField(\"route\", StringType(), True),\n",
        "    StructField(\"direction\", StringType(), True),\n",
        "    StructField(\"next_stop\", StringType(), True),\n",
        "    StructField(\"window\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Ler o arquivo Parquet em modo streaming\n",
        "#parquet_df_stream = spark.readStream \\\n",
        "#    .schema(schema) \\\n",
        "#    .parquet(\"output/silver\")\n",
        "\n",
        "# Exibir os primeiros registros (em modo streaming, apenas enquanto processa)\n",
        "#parquet_df_stream.writeStream \\\n",
        "#    .outputMode(\"append\") \\\n",
        "#    .format(\"console\") \\\n",
        "#    .start()\n",
        "\n",
        "\n",
        "\n",
        "time.sleep(240)\n",
        "query_bronze.stop()\n",
        "# Aguarda até o término do streaming\n",
        "#query_bronze.awaitTermination()\n",
        "#query_silver.awaitTermination()\n",
        "# Aguarda até o término do streaming\n",
        "#parquet_df_stream.awaitTermination()\n"
      ],
      "metadata": {
        "id": "eaz-Ephk_5fk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Criar a sessão Spark\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho para o arquivo Parquet específico\n",
        "file_path = \"/content/output/silver/part-00000-2b2120f2-6cb2-4270-9ace-cd8e76523b0f-c000.snappy.parquet\"\n",
        "\n",
        "# Ler o arquivo Parquet específico\n",
        "parquet_df = spark.read.parquet(file_path)\n",
        "\n",
        "# Exibir os primeiros registros\n",
        "parquet_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "eQm9NbJxLwUD",
        "outputId": "b9525745-3b1e-408d-e130-c51e3b3ba6bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[COLUMN_ALREADY_EXISTS] The column `data_points` already exists. Consider to choose another name or rename the existing column.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-81d4d804aea0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Ler o arquivo Parquet específico\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mparquet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Exibir os primeiros registros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [COLUMN_ALREADY_EXISTS] The column `data_points` already exists. Consider to choose another name or rename the existing column."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "\n",
        "# Caminho para o arquivo Parquet\n",
        "file_path = \"/content/output/silver/part-00000-a181ef3e-f1ae-4aa8-bcbf-d8e667a800f6-c000.snappy.parquet\"\n",
        "\n",
        "# Ler o arquivo Parquet\n",
        "parquet_file = pq.ParquetFile(file_path)\n",
        "\n",
        "# Exibir o esquema completo para melhor compreensão\n",
        "print(parquet_file.schema)\n",
        "\n",
        "# Ler o conteúdo do arquivo\n",
        "table = parquet_file.read()\n",
        "\n",
        "# Converter para DataFrame do Pandas e exibir os dados\n",
        "df = table.to_pandas()\n",
        "\n",
        "# Exibir as primeiras linhas\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8evEGGUAO5EA",
        "outputId": "fcbe4379-3c99-4b7c-917d-0a1646f38044"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyarrow._parquet.ParquetSchema object at 0x7d8f31c61dc0>\n",
            "required group field_id=-1 spark_schema {\n",
            "  optional binary field_id=-1 id (String);\n",
            "  required group field_id=-1 window {\n",
            "    optional int96 field_id=-1 start;\n",
            "    optional int96 field_id=-1 end;\n",
            "  }\n",
            "  optional double field_id=-1 start_lat;\n",
            "  optional double field_id=-1 start_lon;\n",
            "  optional double field_id=-1 end_lat;\n",
            "  optional double field_id=-1 end_lon;\n",
            "  required int64 field_id=-1 data_points;\n",
            "  optional double field_id=-1 distance;\n",
            "  optional double field_id=-1 average_speed_kmh;\n",
            "  optional binary field_id=-1 line (String);\n",
            "  optional binary field_id=-1 route (String);\n",
            "  optional int64 field_id=-1 direction;\n",
            "  optional binary field_id=-1 next_stop (String);\n",
            "  required int64 field_id=-1 data_points;\n",
            "}\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [id, window, start_lat, start_lon, end_lat, end_lon, data_points, distance, average_speed_kmh, line, route, direction, next_stop, data_points]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import from_unixtime,timestamp_seconds, udf, col, lag, radians, sin, cos, sqrt, atan2\n",
        "\n",
        "def transform_data():\n",
        "    # Adicionar coluna transformada\n",
        "    batch_df = batch_df.withColumn(\"timestamp\", timestamp_seconds(col(\"timestamp\")))\n",
        "\n",
        "    # Criar janela para calcular lag\n",
        "    vehicle_window = Window.partitionBy(\"id\").orderBy(\"timestamp\")\n",
        "\n",
        "\n",
        "    # Adicionar colunas de latitude e longitude anteriores\n",
        "    batch_df = batch_df.withColumn(\"prev_lat\", lag(\"lat\").over(vehicle_window))\n",
        "    batch_df = batch_df.withColumn(\"prev_lon\", lag(\"lon\").over(vehicle_window))\n",
        "\n",
        "    # Escrever o resultado no diretório em JSON\n",
        "    batch_df.write.mode(\"append\").format(\"parquet\").save(\"/content/silver/vehicles\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WO9FiSeJY1ma"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform transformations to calculate metrics\n",
        "agg_df = df.groupBy(\"route_id\").agg(\n",
        "    avg(\"speed\").alias(\"average_speed\"),\n",
        "    count(\"trip_id\").alias(\"number_of_trips\"),\n",
        "    sum(lit(2) * col(\"speed\")).alias(\"total_distance_km\"),  # Approximation assuming speed is in km/h\n",
        "    sum(col(\"timestamp\")).alias(\"total_travel_time\")\n",
        ")\n",
        "\n",
        "# Display aggregated data to console\n",
        "query = agg_df.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "# Path of folder to keep aggregated output\n",
        "#aggregated_output_path = \"gs://edit-data-eng-project-group2/aggregated_metrics/\""
      ],
      "metadata": {
        "id": "w4z4vJqB_c3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 5) \\\n",
        "    .load(\"/content/output/\")"
      ],
      "metadata": {
        "id": "fsktG9fgZ9PD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (df.writeStream\n",
        "         .outputMode('append')\n",
        "         .format('parquet')\n",
        "         .option('checkpointLocation', '/content/silver/vehicles_checkpoint')\n",
        "         .trigger(processingTime='5 seconds')\n",
        "         .foreachBatch(transform_data)\n",
        "         .start()\n",
        ")"
      ],
      "metadata": {
        "id": "WxF6t4j1ZquN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKnSRaOcZjEe",
        "outputId": "8b54173f-bd74-4b2f-8859-6da8c69d010e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = spark.read.format(\"parquet\").load(\"/content/output/bronze\")\n",
        "output_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K0coKZbZ0sd",
        "outputId": "f7d8059d-4ce6-423e-c9c3-6be606642d16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------------+------------------+----+------+---------+---------+\n",
            "|      id|              window|            distance| average_speed_kmh|line| route|direction|next_stop|\n",
            "+--------+--------------------+--------------------+------------------+----+------+---------+---------+\n",
            "| 41|1125|{2025-01-21 17:56...| 0.23853682262448198|  7.15610467873446|1704|1704_0|      217|   030783|\n",
            "| 41|1349|{2025-01-21 17:56...| 0.22316170332161805| 6.694851099648542|1527|1527_0|      303|   120279|\n",
            "| 41|1401|{2025-01-21 17:56...| 0.28088922979385883| 8.426676893815765|1254|1254_0|      146|   171461|\n",
            "| 41|1800|{2025-01-21 17:56...| 0.05519986952938248|1.6559960858814744|1229|1229_0|        0|   170971|\n",
            "|  41|335|{2025-01-21 17:56...| 0.29367113808482587| 8.810134142544776|1719|1719_0|      230|   030626|\n",
            "| 42|2018|{2025-01-21 17:56...|                 0.0|               0.0|2926|2926_0|      249|   200002|\n",
            "| 42|2300|{2025-01-21 17:56...|                 0.0|               0.0|2921|2921_0|        0|   200031|\n",
            "| 42|2401|{2025-01-21 17:56...| 0.23263692405020425| 6.979107721506128|2790|2790_1|      203|   180551|\n",
            "| 42|2557|{2025-01-21 17:56...|  0.2053118373468439| 6.159355120405317|2632|2632_0|       78|   110270|\n",
            "| 43|2003|{2025-01-21 17:56...|                 0.0|               0.0|3121|3121_0|        0|   140671|\n",
            "| 43|2104|{2025-01-21 17:56...|  0.4645187646026546|13.935562938079638|3535|3535_0|       97|   150545|\n",
            "| 43|2205|{2025-01-21 17:56...| 0.09232843681256832|2.7698531043770496|3018|3018_0|      284|   020617|\n",
            "| 43|2287|{2025-01-21 17:56...| 0.06156241464031483|1.8468724392094449|3024|3024_0|      240|   020075|\n",
            "|44|12059|{2025-01-21 17:56...| 0.22266412855549042| 6.679923856664713|4428|4428_0|      196|   160423|\n",
            "| 41|1232|{2025-01-21 17:56...|  0.4669532754229987|14.008598262689961|1626|1626_0|      200|   050196|\n",
            "| 41|1241|{2025-01-21 17:56...| 0.32930149722225277| 9.879044916667583|1614|1614_1|      338|   050124|\n",
            "| 41|1264|{2025-01-21 17:56...| 0.22078293253886672| 6.623487976166001|1213|1213_0|      355|   170443|\n",
            "| 41|1318|{2025-01-21 17:56...| 0.08523417382278425|2.5570252146835277|1716|1716_0|      332|   170596|\n",
            "| 41|1371|{2025-01-21 17:56...|0.051347058722088945|1.5404117616626685|1717|1717_0|        0|   170924|\n",
            "| 41|1393|{2025-01-21 17:56...| 0.06435321452435844|1.9305964357307532|1624|1624_1|        0|   050298|\n",
            "+--------+--------------------+--------------------+------------------+----+------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to file in colab\n",
        "file_path = \"/content/output/part-00000-fc32eaa6-deef-4f2c-b72b-2009f077bb01-c000.json\"\n",
        "\n",
        "# Loading Json into Dataframe Spark\n",
        "df = spark.read.json(file_path)\n",
        "\n",
        "# Showing schema of dataframe\n",
        "df.printSchema()\n",
        "\n",
        "# Showing data\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7ocyYXO_bqg",
        "outputId": "71a6c434-0c0e-4cd6-f52c-42f6c8bdee77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- bearing: long (nullable = true)\n",
            " |-- block_id: string (nullable = true)\n",
            " |-- current_status: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- line_id: string (nullable = true)\n",
            " |-- lon: double (nullable = true)\n",
            " |-- pattern_id: string (nullable = true)\n",
            " |-- route_id: string (nullable = true)\n",
            " |-- schedule_relationship: string (nullable = true)\n",
            " |-- shift_id: string (nullable = true)\n",
            " |-- speed: double (nullable = true)\n",
            " |-- stop_id: string (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- trip_id: string (nullable = true)\n",
            "\n",
            "+-------+--------------------+--------------+--------+------------------+-------+------------------+----------+--------+---------------------+------------+------------------+-------+----------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|               lat|line_id|               lon|pattern_id|route_id|schedule_relationship|    shift_id|             speed|stop_id| timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+------------------+-------+------------------+----------+--------+---------------------+------------+------------------+-------+----------+--------------------+\n",
            "|    277|20250103-64410439...| IN_TRANSIT_TO|44|12747| 38.76730728149414|   4720|-9.102219581604004|  4720_0_1|  4720_0|            SCHEDULED|213030234560| 9.166666666666666| 160027|1735886804|4720_0_1|1400|064...|\n",
            "|    171|20250103-64410129...| IN_TRANSIT_TO|44|12645| 38.70551300048828|   4514|-8.975939750671387|  4514_0_2|  4514_0|            SCHEDULED|221350234560|               0.0| 100013|1735886804|4514_0_2|1400|070...|\n",
            "|    274|20250103-64410037...| IN_TRANSIT_TO|44|12682|38.717777252197266|   4701|-8.945494651794434|  4701_2_2|  4701_2|            SCHEDULED|223150234560| 13.88888888888889| 060005|1735886804|4701_2_2|1400|062...|\n",
            "|    212|20250103-64410088...| IN_TRANSIT_TO|44|12734|38.527305603027344|   4421|-8.905706405639648|  4421_0_2|  4421_0|            SCHEDULED|212170234560|               0.0| 160442|1735886811|4421_0_2|1400|064...|\n",
            "|      7|20250103-64410087...| IN_TRANSIT_TO|44|12674| 38.62263107299805|   4720|-8.867168426513672|  4720_0_2|  4720_0|            SCHEDULED|213090234560| 27.77777777777778| 060005|1735886817|4720_0_2|1400|063...|\n",
            "|    353|20250103-64410028...| IN_TRANSIT_TO|44|12687| 38.70286178588867|   4707|-8.947881698608398|  4707_0_1|  4707_0|            SCHEDULED|223070234560| 13.88888888888889| 100040|1735886803|4707_0_1|1400|063...|\n",
            "|    317|20250103-64410010...| IN_TRANSIT_TO|44|12514| 38.70011901855469|   4600|-8.957487106323242|  4600_0_2|  4600_0|            SCHEDULED|221070234560|               0.0| 100008|1735886813|4600_0_2|1400|060...|\n",
            "|     80|20250103-64410079...| IN_TRANSIT_TO|44|12560| 38.53632354736328|   4551|-8.890315055847168|  4551_0_2|  4551_0|            SCHEDULED|212140234560|10.833333333333334| 160970|1735886804|4551_0_2|1400|063...|\n",
            "|    155|20250103-64410008...|   INCOMING_AT|44|12504|38.713294982910156|   4504|-8.900853157043457|  4504_0_2|  4504_0|            SCHEDULED|221050234560|11.666666666666666| 010123|1735886817|4504_0_2|1400|063...|\n",
            "|    301|20250103-64410001...| IN_TRANSIT_TO|44|12531| 38.75092315673828|   4504|-8.941862106323242|  4504_0_2|  4504_0|            SCHEDULED|221010234560|               0.0| 010136|1735886805|4504_0_2|1400|070...|\n",
            "|      0|                1256|    STOPPED_AT| 42|1256| 38.75950622558594|   2776| -9.15932559967041|  2776_1_2|  2776_1|            SCHEDULED|       44123|               0.0| 060341|1735886727|2776_1_2|150|2|06...|\n",
            "|    283|           4_4012-12|   INCOMING_AT| 41|1717| 38.81306838989258|   1636|-9.251324653625488|  1636_1_1|  1636_1|            SCHEDULED|        4012|              10.0| 171577|1735886815|1636_1_1_0630_065...|\n",
            "|    292|           4_4010-12|   INCOMING_AT| 41|1855| 38.75532150268555|   1720|-9.215667724609375|  1720_0_2|  1720_0|            SCHEDULED|        4010|10.555555555555555| 030303|1735886809|1720_0_2_0630_065...|\n",
            "|    344|20250103-64410064...| IN_TRANSIT_TO|44|12694| 38.71299362182617|   4707|-8.938298225402832|  4707_0_2|  4707_0|            SCHEDULED|223190234560| 13.61111111111111| 060005|1735886805|4707_0_2|1400|063...|\n",
            "|    275|20250103-64410115...| IN_TRANSIT_TO|44|12744|  38.5853385925293|   4725|-8.913541793823242|  4725_0_2|  4725_0|            SCHEDULED|213100234560|              27.5| 020003|1735886804|4725_0_2|1400|063...|\n",
            "|    247|20250103-64410193...| IN_TRANSIT_TO|44|12562|38.521568298339844|   4412|-8.821974754333496|  4412_0_1|  4412_0|            SCHEDULED|211370234560|0.8333333333333333| 160324|1735886806|4412_0_1|1400|064...|\n",
            "|    212|           4_4319-12|    STOPPED_AT| 41|1391|  38.7177848815918|   1608|-9.324108123779297|  1608_0_2|  1608_0|            SCHEDULED|        4319|3.0555555555555554| 050077|1735886815|1608_0_2_0630_065...|\n",
            "|     11|           4_4615-12| IN_TRANSIT_TO| 41|1378|38.766841888427734|   1513|-9.257182121276855|  1513_0_3|  1513_0|            SCHEDULED|        4615| 5.833333333333333| 172111|1735886814|1513_C_0_3_0630_0...|\n",
            "|      8|20250103-64410086...| IN_TRANSIT_TO|44|12683|38.585716247558594|   4710|-8.890913009643555|  4710_0_2|  4710_0|            SCHEDULED|213110234560|  8.61111111111111| 130210|1735886805|4710_0_2|1400|064...|\n",
            "|    197|20250103-64410069...|   INCOMING_AT|44|12556|38.657135009765625|   4524|-8.740750312805176|  4524_0_2|  4524_0|            SCHEDULED|211130234560|15.555555555555555| 130153|1735886809|4524_0_2|1400|063...|\n",
            "+-------+--------------------+--------------+--------+------------------+-------+------------------+----------+--------+---------------------+------------+------------------+-------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}