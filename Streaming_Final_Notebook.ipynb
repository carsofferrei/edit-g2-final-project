{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/edit-g2-final-project/blob/streaming_measures_calculations/Streaming_Final_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Import Libraries**"
      ],
      "metadata": {
        "id": "DXgJYVg0wSqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
        "from pyspark.sql.functions import col, count, sum, lit, window, first, sqrt, pow, last, when\n",
        "from pyspark.sql.window import Window\n",
        "import time"
      ],
      "metadata": {
        "id": "b3HxQo2XZX7y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Set Up Environment**"
      ],
      "metadata": {
        "id": "zDVK46WfZ09F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages and dependencies\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install gcsfs -q\n",
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/\n",
        "!gcloud auth application-default login\n",
        "\n",
        "# Remove all files in the /content/output directory to clean up before processing\n",
        "!rm -rf /content/output/*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvsYhdONZ4AW",
        "outputId": "355f6711-5349-4bfb-d428-708fd7f9693d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2025-01-24 18:56:06--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  32.26M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-24 18:56:07 (278 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’ saved [33831577/33831577]\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=FGOdEQXWytDtLIPwljXcynJAqkZkjb&prompt=consent&token_usage=remote&access_type=offline&code_challenge=qjQJylopwXiJPeMstJoha-I-9uyqi0netX2UAk8WXxE&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0ASVgi3Kc8IYzJ9k8lHBs78FO80jkKNubOA7O9gEs5PlrIpRWZTVzJ4iPeCvpN3fqGjWSEQ\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Create Spark Session and Configure GCS**"
      ],
      "metadata": {
        "id": "TPENZj6iZ6Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Spark session with the GCS connector\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCSReadVehicles\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path to the credentials file generated by gcloud\n",
        "credential_path = \"/content/.config/application_default_credentials.json\"\n",
        "\n",
        "# Set up the environment for credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_path\n",
        "\n",
        "# Configure PySpark to access GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", credential_path)\n"
      ],
      "metadata": {
        "id": "-WgLaNN8Z-Ug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Define Input Path and Schema**\n"
      ],
      "metadata": {
        "id": "VqPojnIbaCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCS bucket path\n",
        "bucket_path = \"gs://edit-de-project-streaming-data/carris-vehicles/\"\n",
        "\n",
        "# Define schema for JSON data\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", LongType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", DoubleType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "SbNO4TjzaFLq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Read Streaming Data**\n"
      ],
      "metadata": {
        "id": "Gwg7ZLpNaJae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from GCS in streaming mode with a predefined schema\n",
        "df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 5) \\\n",
        "    .load(bucket_path)\n",
        "\n",
        "# Path to store the processed output\n",
        "bronze_output_path = \"output/bronze\"\n",
        "\n",
        "# Ensure the timestamp column is in the correct format\n",
        "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n"
      ],
      "metadata": {
        "id": "orv-ceSuaNAJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Perform Aggregations**\n"
      ],
      "metadata": {
        "id": "cothNFF3aQHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a 2-minute window for aggregations\n",
        "agg_distance = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"lat\").alias(\"start_lat\"),\n",
        "        first(\"lon\").alias(\"start_lon\"),\n",
        "        last(\"lat\").alias(\"end_lat\"),\n",
        "        last(\"lon\").alias(\"end_lon\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "# Calculate distance using the Euclidean distance formula between the first and last points\n",
        "agg_distance = agg_distance.withColumn(\n",
        "    \"distance\",\n",
        "    sqrt(pow(col(\"end_lat\") - col(\"start_lat\"), 2) + pow(col(\"end_lon\") - col(\"start_lon\"), 2)) * lit(111)  # Approximation for km\n",
        ")\n",
        "\n",
        "# Calculate the average speed (km/h) based on distance and time\n",
        "agg_speed = agg_distance.withColumn(\n",
        "    \"average_speed_kmh\",\n",
        "    (col(\"distance\") / lit(2 / 60))  # Convert 2 minutes to hours\n",
        ")\n",
        "\n",
        "# Add columns to define current_stop or next_stop based on current_status\n",
        "df = df.withColumn(\n",
        "    \"stop\",\n",
        "    when(col(\"current_status\") == \"stopped_at\", col(\"stop_id\")).otherwise(lit(\"-1\"))\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"next_stop\",\n",
        "    when(col(\"current_status\") != \"stopped_at\", col(\"stop_id\")).otherwise(lit(\"-1\"))\n",
        ")\n",
        "\n",
        "# Aggregate vehicle attributes in the same 2-minute window\n",
        "windowed_df = df.withWatermark(\"timestamp\", \"2 minutes\") \\\n",
        "    .groupBy(\"id\", window(col(\"timestamp\"), \"2 minutes\")) \\\n",
        "    .agg(\n",
        "        first(\"line_id\").alias(\"line\"),\n",
        "        first(\"route_id\").alias(\"route\"),\n",
        "        first(\"bearing\").alias(\"direction\"),\n",
        "        first(\"stop\").alias(\"current_stop\"),\n",
        "        first(\"next_stop\").alias(\"next_stop\"),\n",
        "        count(\"*\").alias(\"data_points\")\n",
        "    )\n",
        "\n",
        "# Drop unnecessary columns\n",
        "windowed_df = windowed_df.drop(\"data_points\")"
      ],
      "metadata": {
        "id": "yJQxSuxaaS5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Join Data and Write to Bronze Layer**"
      ],
      "metadata": {
        "id": "B4wg1yEjaWx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join speed and distance data with vehicle attributes\n",
        "final_result = agg_speed.join(windowed_df, [\"id\", \"window\"], \"inner\") \\\n",
        "    .drop(\"data_points\", \"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\")\n",
        "\n",
        "# Write the results to the Bronze layer\n",
        "query_bronze = final_result.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", bronze_output_path) \\\n",
        "    .option(\"checkpointLocation\", f\"{bronze_output_path}/_checkpoint\") \\\n",
        "    .start()\n",
        "\n",
        "# Allow the streaming query to run for 240 seconds\n",
        "time.sleep(240)\n",
        "query_bronze.stop()"
      ],
      "metadata": {
        "id": "wgFKvaDQaZT_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Optional: Write Data to GCS**\n"
      ],
      "metadata": {
        "id": "6XgDF05datui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To save data in the bucket\n",
        "def write_to_gcs(batch_df, batch_id):\n",
        "    batch_df.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .format(\"parquet\") \\\n",
        "        .option(\"path\", \"gs://edit-data-eng-project-group2/vehicles_data/\") \\\n",
        "        .save()\n",
        "\n",
        "query_bronze = final_result.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .foreachBatch(write_to_gcs) \\\n",
        "    .option(\"checkpointLocation\", \"gs://edit-data-eng-project-group2/vehicles_data/_checkpoint\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .start()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "mDkxdElsai8j",
        "outputId": "4931c17a-7c57-43d3-9963-1822211f62d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTo save data in the bucket\\ndef write_to_gcs(batch_df, batch_id):\\n    batch_df.write         .mode(\"append\")         .format(\"parquet\")         .option(\"path\", \"gs://edit-data-eng-project-group2/vehicles_data/\")         .save()\\n\\nquery_bronze = final_result.writeStream     .outputMode(\"append\")     .foreachBatch(write_to_gcs)     .option(\"checkpointLocation\", \"gs://edit-data-eng-project-group2/vehicles_data/_checkpoint\")     .trigger(processingTime=\"30 seconds\")     .start()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Read and Display Data from Bronze Layer**"
      ],
      "metadata": {
        "id": "uikvka7DaeaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_df = spark.read.format(\"parquet\").load(\"/content/output/bronze\")\n",
        "output_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AmFuLIQtMF8",
        "outputId": "57bf0530-415c-42d8-b1d5-21229d08c186"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------+-----------------+----+-----+---------+------------+---------+\n",
            "| id|window|distance|average_speed_kmh|line|route|direction|current_stop|next_stop|\n",
            "+---+------+--------+-----------------+----+-----+---------+------------+---------+\n",
            "+---+------+--------+-----------------+----+-----+---------+------------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}